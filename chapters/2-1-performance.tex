\subsection{Performance}

In computing performance can be defined by two factors,
computational requirements or resources.
Computational requirements can be thought of "what needs to be done?", or efficacy,
and computational resources can be thought in terms of "how much will it cost?", or efficiency.

\begin{equation}
    \mathnormal{Performace} \sim \frac{1}{\mathnormal{Resources~for~solution}}
\end{equation}

\subsubsection{Expectations}
In a scenario where we have $p$ processors and each processor is rated at $f$ MFLOP, should we see $f \times p$ MFLOPS performance?

The answer is not as simple as "yes".
Several causes may affect performance, while causes may interact with each other,
they need to be understood separately.

\subsubsection{Embarrassingly Parallel Computations}
Or for short EPCs, are computations that can be trivially divided into several independent parts able to be executed simultaneously.
For \textit{truly} EPCs there should be no interaction between processes, while in \textit{nearly} EPCs the input and output is required to be distributed and combined in some way.

EPCs have potential to achieve maximal speedups in parallel platforms.

\subsubsection{Latency \& Throughput}

\paragraph{Latency}
The time it takes to complete a task is called latency.
The scale can be anywhere from the nanoseconds to days, lower latency is better.
It is related to \textit{response time}.

\paragraph{Throughput}
The rate at which a series of tasks can be completed is called throughput.
It has units of work per time unit, a larger throughput is better.

\subsubsection{Speedup, Efficiency \& Scalability}
Speedup and efficiency are important metrics for parallelism relating to performance.

\paragraph{Speedup}
Speedup compares the latency for solving the identical computational problem on one hardware unit versus on $P$ hardware units.
\begin{equation}\label{eq:speedup}
    S_p = \frac{T_1}{T_p}
\end{equation}
Where $T_1$ is the latency of the program with one worker and $T_P$ is the latency on $P$ workers.

\paragraph{Efficiency}
It is the speedup divided by the number of workers.
\begin{equation}\label{eq:efficiency}
    E_p = \frac{S_p}{p}
\end{equation}
It measures the return on hardware investment.
The ideal efficiency is $1$, corresponding to a linear speedup.

\paragraph{Cost}
A parallel algorithm is cost-optimal if $C_p = T_1$ or equivalently $E_p = 1$.
\begin{equation}\label{eq:cost}
    C_p = p \times T_p
\end{equation}

\paragraph{Scalability}
As previously discussed the performance of a parallel solution is subjected to several factors, namely scalability.
Scalability is the ability of a parallel algorithm of achieving performance gains proportional to the number of processors and the size of the problem.

\subsubsection{Asymptotic Complexity}
Asymptotic complexity is the key to comparing algorithms.
Comparing absolute times is not particularly meaningful since they are specific to the hardware.

\paragraph{Time Complexity}
An algorithm's time complexity summarizes how the execution time of algorithm grows with the input size.

\paragraph{Space Complexity}
Similarly to time complexity, space complexity summarizes how the required amount of memory grows with input size.

\paragraph{Big O Notation}
Denotes a set of functions with an upper bound.
$O(f(N))$ is the set of all functions $g(N)$ such that there exist positive constants $c$ and $N_0$ with $|g(N)| \le c \cdot f(N)$ for $N \ge N_0$.

\paragraph{Big $\Omega$ Notation}
Denotes a set of functions with a lower bound.
$\Omega(f(N))$ is the set of all functions $g(N)$ such that there exist constants $c$ and $N_0$ with $g(N) \ge c \cdot f(N)$ for $N \ge N_0$.

\paragraph{Big $\Theta$ Notation}
Denotes a set of functions with both upper and lower bounds.
$\Theta(f(N))$ means the set of all functions $g(N)$ such that there exist positive constants $c_1$, $c_2$ and $N_0$ with
$c_1 \cdot f(N) \le g(N) \le c_2 \cdot f(N)$ for $N \ge N_0$.